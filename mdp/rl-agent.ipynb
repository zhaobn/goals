{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Hashable, TypeVar, Generic, Container, Any, Union, Counter\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "import random\n",
    "from random import Random\n",
    "from collections import defaultdict, Counter\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patheffects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want actions to be hashable so that we can use them as dictionary keys for Q-learning later\n",
    "Action = TypeVar('Action', bound=Hashable)\n",
    "State = TypeVar('State', bound=Hashable)\n",
    "\n",
    "@dataclass(frozen=True, eq=True, repr=True)\n",
    "class Step:\n",
    "    '''For keeping track of trajectories'''\n",
    "    state : State\n",
    "    action : Action\n",
    "    reward : float\n",
    "    next_state : State\n",
    "\n",
    "Trajectory = Sequence[Step]\n",
    "\n",
    "class MarkovDecisionProcess(Generic[State, Action]):\n",
    "    '''A general class for many MDPs.'''\n",
    "    discount_rate : float\n",
    "    action_space : Sequence[Action]\n",
    "    state_space : Sequence[State]\n",
    "    state_action_space : Sequence[tuple[State, Action]]\n",
    "\n",
    "    def actions(self, s : State) -> Sequence[Action]:\n",
    "        '''Return the action space.'''\n",
    "        return self.action_space\n",
    "    \n",
    "    def next_state_sample(self, s : State, a : Action, rng : Random = random) -> State:\n",
    "        '''Sample the next state.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reward(self, s : State, a : Action, ns : State) -> float:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def is_absorbing(self, s : State) -> bool:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class MDPPolicy(Generic[State, Action]):\n",
    "    '''A very general class for an MDP policy.'''\n",
    "    DISCOUNT_RATE : float\n",
    "\n",
    "    def action_sample(self, s : State, rng : Random = random) -> Action:\n",
    "        '''Sample which actions to take.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def state_value(self, s : State) -> float:\n",
    "        '''Retrieve the value of a state: s.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, s : State, a : Action, r : float, ns : State) -> None:\n",
    "        '''Update the value of a state action pair.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def end_episode(self) -> None:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape World Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the typing issue that makes something later give some callable error\n",
    "Shape = namedtuple('Shape',['sides', 'shade', 'texture'])\n",
    "State = namedtuple('State',['shape1', 'shape2', 'shape3'])\n",
    "Action = namedtuple('Action',['actor','recipient'])\n",
    "\n",
    "# The action space. a = actor, r = recipient\n",
    "a1r2 = Action(0, 1)\n",
    "a1r3 = Action(0, 2)\n",
    "a2r1 = Action(1, 0)\n",
    "a2r3 = Action(1, 2)\n",
    "a3r1 = Action(2, 0)\n",
    "a3r2 = Action(2, 1)\n",
    "\n",
    "class ShapeWorld(MarkovDecisionProcess[State, Action]):\n",
    "    GOAL = None\n",
    "    GOAL_REWARD = 100\n",
    "    STEP_COST = -1\n",
    "    SHAPE_LIST = tuple(['circle','square','triangle'])\n",
    "    SHADE_LIST = tuple(['low','medium','high'])\n",
    "    TEXTURE_LIST = tuple(['present','not_present'])\n",
    "    SHAPE_TRANSITION_PROB = 0.8\n",
    "\n",
    "    def __init__(self, goal : State, discount_rate):\n",
    "        self.discount_rate = discount_rate\n",
    "        self.GOAL = goal\n",
    "\n",
    "        # set up shapeworld shape space\n",
    "        shape_space : Sequence[Shape] = tuple(\n",
    "            Shape(sides, shade, texture) \n",
    "                for (sides, shade, texture) \n",
    "                    in product(self.SHAPE_LIST,self.SHADE_LIST,self.TEXTURE_LIST)\n",
    "        )\n",
    "\n",
    "        # set up state space\n",
    "        self.state_space : Sequence[State] = tuple(\n",
    "            State(shape1, shape2, shape3) for(shape1, shape2, shape3) in product(shape_space, shape_space, shape_space)\n",
    "        )\n",
    "\n",
    "        # set up state action space\n",
    "        self.action_space = tuple([a1r2, a1r3, a2r1, a2r3, a3r1, a3r2])\n",
    "        self.state_action_space = tuple(product(self.state_space, self.action_space))\n",
    "\n",
    "    def actions(self, s : State) -> Sequence[Action]:\n",
    "        '''Return the action space.'''\n",
    "        return self.action_space\n",
    "    \n",
    "    def next_state_sample(\n",
    "            self,\n",
    "            s : State,\n",
    "            a : Action,\n",
    "            rng : random.Random = random\n",
    "    ) -> State:\n",
    "        # determine if recipient shape changes\n",
    "        if rng.random() < self.SHAPE_TRANSITION_PROB:\n",
    "            sides = s[a.actor].sides\n",
    "        else:\n",
    "            sides = s[a.recipient].sides\n",
    "        \n",
    "        # determine if the recipient shade changes\n",
    "        shade = None\n",
    "\n",
    "        if self._is_darker(s[a.actor].shade, s[a.recipient].shade):\n",
    "            shade = self._get_darker_shade(s[a.recipient].shade)\n",
    "\n",
    "        elif self._is_lighter(s[a.actor].shade, s[a.recipient].shade):\n",
    "            shade = self._get_lighter_shade(s[a.recipient].shade)\n",
    "\n",
    "        else:\n",
    "            shade = s[a.recipient].shade\n",
    "\n",
    "        # determine recipient texture change\n",
    "        texture = 'not_present' if s[a.recipient].texture == 'present' else 'present'\n",
    "\n",
    "        # instantiate next state\n",
    "        new_state_list  = [s[0], s[1], s[2]]\n",
    "        new_state_list[a.recipient] = Shape(sides, shade, texture)\n",
    "        new_state = tuple(new_state_list)\n",
    "\n",
    "        return new_state\n",
    "    \n",
    "    def reward(self, s: State, a : Action, ns : State) -> float:\n",
    "        reward = self.STEP_COST\n",
    "        if self._is_goal(ns):\n",
    "            reward += self.GOAL_REWARD\n",
    "        return reward\n",
    "\n",
    "    def is_absorbing(self, s: State) -> bool:\n",
    "        return False\n",
    "\n",
    "    def plot(self, ax=None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # helper functions\n",
    "    def _is_goal(self, ns : State):\n",
    "        return self.GOAL == ns\n",
    "\n",
    "    def _is_darker(self, shade1, shade2):\n",
    "        return self.SHADE_LIST.index(shade1) > self.SHADE_LIST.index(shade2)\n",
    "    \n",
    "    def _is_lighter(self, shade1, shade2):\n",
    "        return self.SHADE_LIST.index(shade1) < self.SHADE_LIST.index(shade2)\n",
    "    \n",
    "    def _get_darker_shade(self, shade):\n",
    "        current_idx = self.SHADE_LIST.index(shade)\n",
    "        return self.SHADE_LIST[current_idx + 1] if current_idx > 0 else shade\n",
    "    \n",
    "    def _get_lighter_shade(self, shade):\n",
    "        current_idx = self.SHADE_LIST.index(shade)\n",
    "        return self.SHADE_LIST[current_idx - 1] if current_idx < len(self.SHADE_LIST) else shade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(shape1=Shape(sides='circle', shade='medium', texture='present'), shape2=Shape(sides='circle', shade='medium', texture='present'), shape3=Shape(sides='circle', shade='medium', texture='present'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make shape world\n",
    "shape1 = Shape(sides='circle', shade='medium', texture='present')\n",
    "shape2 = Shape(sides='square', shade='low', texture='present')\n",
    "goal = State(shape2, shape1, shape1)\n",
    "shape_world = ShapeWorld(goal, discount_rate=0.8)\n",
    "\n",
    "# Test the transition function\n",
    "shapeA = Shape(sides='circle', shade='medium', texture='present')\n",
    "shapeB = Shape(sides='square', shade='low', texture='present')\n",
    "s0 = State(shapeA, shapeA, shapeA)\n",
    "s0\n",
    "\n",
    "# Check out the state space\n",
    "#shape_world.state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLearner(MDPPolicy):\n",
    "    def __init__(self, discount_rate, learning_rate, initial_value, epsilon, action_space):\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_value = initial_value\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Using default dict allows us to set a default value for \n",
    "        items that do not have a value assigned to them yet. \n",
    "        Useful for updating values for state-action pairs for states\n",
    "        that have not been encountered yet. \n",
    "        '''\n",
    "        self.estimated_state_action_values = defaultdict(\n",
    "            lambda : {a: self.initial_value for a in self.action_space}\n",
    "        )\n",
    "\n",
    "    def state_value(self, s) -> float:\n",
    "        return max(self.estimated_state_action_values[s].values())\n",
    "    \n",
    "    def action_sample(self, s, rng = random):\n",
    "        # e-greedy action selection\n",
    "        if rng.random() < self.epsilon:\n",
    "            return rng.choice(self.action_space)\n",
    "        astar = max(self.estimated_state_action_values[s].items(),\n",
    "                          key = lambda x: x[1])[0]\n",
    "        return astar\n",
    "\n",
    "class QLearner(BaseLearner):\n",
    "    def end_episode(self):\n",
    "        pass\n",
    "\n",
    "    def update(self, s, a, r, ns):\n",
    "        max_ns_q = max(self.estimated_state_action_values[ns].values())\n",
    "        td_target = r + self.discount_rate*max_ns_q\n",
    "        td_error = td_target - self.estimated_state_action_values[s][a]\n",
    "\n",
    "        # TODO: why doesn't estimated_state_action_values appear elsewhere?\n",
    "        self.estimated_state_action_values[s][a] += self.learning_rate * td_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation, Gym, and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = Union[int, np.ndarray, dict[str, Any]]\n",
    "ActType = Union[int, np.ndarray, dict[str, Any]]\n",
    "\n",
    "class GymWrapper(gym.Env):\n",
    "    def __init__(self, mdp : MarkovDecisionProcess):\n",
    "        self.mdp = mdp\n",
    "        self.current_state = None\n",
    "        self.rng : random.Random = random\n",
    "        self.action_space = gym.spaces.Discrete(len(mdp.action_space))\n",
    "        assert not isinstance(next(iter(mdp.action_space)), int), \\\n",
    "            \"To avoid ambiguity with gym action encoding, action space must not be an integer\"\n",
    "        self.observation_space = gym.spaces.Discrete(len(mdp.state_space))\n",
    "        self.reward_range = (float('-inf'), float('inf'))\n",
    "\n",
    "    def step(self, action : ActType) -> tuple[ObsType, float, bool, dict[str, Any]]:\n",
    "        '''returns: obs, reward, terminated, info'''\n",
    "        assert isinstance(action, int), 'Input is the action index'\n",
    "        action = self.mdp.action_space[action]\n",
    "        next_state = self.mdp.next_state_sample(self.current_state, action, rng=self.rng)\n",
    "        next_state_idx = self.mdp.state_space.index(next_state)\n",
    "        reward = self.mdp.reward(self.current_state, action, next_state)\n",
    "        terminated = self.mdp.is_absorbing(next_state)\n",
    "        self.current_state = next_state\n",
    "        info = dict(\n",
    "            state=self.current_state,\n",
    "            action=action,\n",
    "            next_state=next_state,\n",
    "            reward=reward\n",
    "        )\n",
    "        return next_state_idx, reward, terminated, info\n",
    "    \n",
    "    def reset(self, *, seed : int = None, options : dict[str, Any] = None) -> tuple[ObsType, dict[str, Any]]:\n",
    "        super().reset(seed=seed)\n",
    "        self.rng = random.Random(seed)\n",
    "        # TODO: fix this to where the starts really are random\n",
    "        shape1 = Shape(sides='circle', shade='medium', texture='present')\n",
    "        shape2 = Shape(sides='square', shade='low', texture='present')\n",
    "        shape3 = Shape(sides='triangle', shade='high', texture='present')\n",
    "        s0 = State(shape1, shape2, shape3)\n",
    "        self.current_state = s0 #self.mdp.initial_state_dist().sample(rng=self.rng)\n",
    "        return self.mdp.state_space.index(self.current_state), dict(state=self.current_state)\n",
    "    \n",
    "    def render(self) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearningSimulationResult:\n",
    "    def __init__(\n",
    "            self,\n",
    "            trajectory,\n",
    "            state_values,\n",
    "            policy : MDPPolicy,\n",
    "            sw : ShapeWorld\n",
    "    ):\n",
    "        self.trajectory = trajectory\n",
    "        self.state_values = state_values\n",
    "        self.policy = policy\n",
    "        self.sw = sw\n",
    "\n",
    "    def plot_timestep(self, timestep):\n",
    "        raise NotImplementedError\n",
    "        # TODO: Create plotting code to visualize what the learner is doing.\n",
    "        timestep = timestep if timestep >= 0 else len(self.trajectory) + timestep\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        states_visited = Counter(\n",
    "            [s for s, _, _, _, _ in self.trajectory[:timestep]]\n",
    "        )\n",
    "        gwp = self.sw.plot(ax=axes[0])\n",
    "        gwp.plot_location_map(states_visited)\n",
    "        gwp.ax.set_title(f\"States Visitation Counts at Timestep {timestep}\")\n",
    "        gwp = self.sw.plot(ax=axes[1])\n",
    "        gwp.plot_location_map(self.state_values[timestep])\n",
    "        gwp.ax.set_title(f\"State Values at Timestep {timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_loop(\n",
    "        *,\n",
    "        env: GymWrapper,\n",
    "        policy : MDPPolicy,\n",
    "        n_episodes : int,\n",
    "        max_steps : int,\n",
    "        seed: int,\n",
    ") -> TDLearningSimulationResult:\n",
    "    \n",
    "    # both initialize and reset the Q values\n",
    "    policy.reset()\n",
    "    trajectory = []\n",
    "    state_values = []\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # episodes loop\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Episodes\"):\n",
    "        # TODO: finish implementing the rest function\n",
    "        state_idx, _ = env.reset(seed=rng.randint(0, 2**32 - 1))\n",
    "        state = env.mdp.state_space[state_idx]\n",
    "\n",
    "        # steps loop\n",
    "        for _ in range(max_steps):\n",
    "            action = policy.action_sample(state, rng=rng)\n",
    "            action_idx = env.mdp.action_space.index(action)\n",
    "            new_state_idx, reward, done, _ = env.step(action_idx)\n",
    "            new_state = env.mdp.state_space[new_state_idx]\n",
    "            policy.update(\n",
    "                s=state,\n",
    "                a=action,\n",
    "                r=reward,\n",
    "                ns=new_state\n",
    "            )\n",
    "\n",
    "            trajectory.append((state, action, reward, new_state, done))\n",
    "            state_values.append({\n",
    "                s: policy.state_value(s)\n",
    "                for s in env.mdp.state_space\n",
    "            })\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        policy.end_episode()\n",
    "    return TDLearningSimulationResult(trajectory, state_values, policy, env.mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 100/100 [00:12<00:00,  8.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make Shape World + Environment/Gym\n",
    "# Make shape world\n",
    "shape1 = Shape(sides='circle', shade='medium', texture='present')\n",
    "shape2 = Shape(sides='square', shade='low', texture='present')\n",
    "goal = State(shape2, shape1, shape1)\n",
    "shape_world = ShapeWorld(goal=goal, discount_rate=0.8)\n",
    "shape_world\n",
    "\n",
    "shape_env = GymWrapper(shape_world)\n",
    "\n",
    "# Make a Q-Learner\n",
    "qlearner = QLearner(\n",
    "    discount_rate = shape_world.discount_rate,\n",
    "    learning_rate = 0.1,\n",
    "    initial_value=0,\n",
    "    epsilon=0.2,\n",
    "    # TODO: Why does other code use mdp.action_space?\n",
    "    action_space=shape_world.action_space\n",
    ")\n",
    "\n",
    "# Specify the simulation parameters\n",
    "results = simulation_loop(\n",
    "    env = shape_env,\n",
    "    policy = qlearner,\n",
    "    n_episodes=100,\n",
    "    max_steps=100,\n",
    "    seed = None\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
