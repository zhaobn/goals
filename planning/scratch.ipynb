{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import rllib\n",
    "from rllib.mdp import OptimalGoalPolicy\n",
    "from rllib.shapeworld import GoalWorld\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from random import sample\n",
    "from math import log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Hashable, TypeVar, Generic, Container, Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import Random\n",
    "import pickle\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "from tqdm import tqdm\n",
    "from typing import TypeVar\n",
    "from rllib.mdp import MarkovDecisionProcess\n",
    "# from .distributions import Distribution, DiscreteDistribution, Uniform, Gaussian\n",
    "\n",
    "# We want actions to be hashable so that we can use them as dictionary keys for Q-learning later\n",
    "Action = TypeVar('Action', bound=Hashable)\n",
    "Shape = namedtuple('Shape',['sides', 'shade', 'texture'])\n",
    "State = TypeVar('State', bound=Hashable)\n",
    "Likelihood = TypeVar('Likelihood', float, int)\n",
    "\n",
    "class GoalSelectionPolicy(Generic[State, Action]):\n",
    "    def __init__(self, mdp: MarkovDecisionProcess[State, Action]):\n",
    "        '''Initialize the policy with the MDP.'''\n",
    "        self.mdp = mdp\n",
    "        self.state_space = mdp.state_space\n",
    "\n",
    "    def sample_action(self, rng : Random = random) -> tuple[Action, Likelihood]:\n",
    "        '''Uses some policy to select which goal to select.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OptimalGoalPolicy(GoalSelectionPolicy[State, Action]):\n",
    "    '''Takes the optimal value function computed from value iteration to select goals.'''\n",
    "\n",
    "    def __init__(self, mdp: MarkovDecisionProcess[State, Action], value_function: dict[State, float]):\n",
    "        super().__init__(mdp)\n",
    "        # we will use the value function from value iteration to select the goal\n",
    "        self.value_function = value_function\n",
    "\n",
    "    def sample_action(self, rng: random.Random = random) -> tuple[State, float]:\n",
    "        '''Use softmax to sample a goal state based on the value function.'''\n",
    "        # Assuming 'actions' here are actually states for goal selection\n",
    "        states = list(self.value_function.keys())\n",
    "        state_values = np.array([self.value_function[state] for state in states])\n",
    "\n",
    "        # Compute softmax probabilities\n",
    "        exp_values = np.exp(state_values - np.max(state_values))  # for numerical stability\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "\n",
    "        # Sample a state based on these probabilities\n",
    "        selected_state = rng.choices(states, weights=probabilities)[0]\n",
    "\n",
    "        # Calculate the negative log likelihood of the selected state\n",
    "        nll = -np.log(probabilities[states.index(selected_state)])\n",
    "\n",
    "        return (selected_state, nll)\n",
    "    \n",
    "    def calc_log_lik(self, state: State) -> float:\n",
    "        '''Calculate the log likelihood of selecting a particular state.'''\n",
    "        state_values = np.array([self.value_function[s] for s in self.state_space])\n",
    "        exp_values = np.exp(state_values - np.max(state_values))\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "        return -np.log(probabilities[self.state_space.index(state)])\n",
    "    \n",
    "    def calc_log_likelihood_all(self) -> dict[State, float]:\n",
    "        '''Calculate the log likelihood of all states in the state space.'''\n",
    "        # get all the value for the states and convert them to probabilities\n",
    "        state_values = np.array([self.value_function[state] for state in self.state_space])\n",
    "        exp_values = np.exp(state_values - np.max(state_values))\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "        # print(f\"Length of probabilities: {len(probabilities)}\")\n",
    "        log_likelihoods = {s: -np.log(probabilities[self.state_space.index(s)]) for s in self.state_space}\n",
    "        return log_likelihoods\n",
    "    \n",
    "    def calc_log_likelihood_all_df(self) -> pd.DataFrame:\n",
    "        '''Return log lik as a dataframe.'''\n",
    "        log_likelihood = self.calc_log_likelihood_all()\n",
    "        return pd.DataFrame(log_likelihood.items(), columns=['State', 'Log Likelihood'])\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal goal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the goal space\n",
    "goal_world = GoalWorld()\n",
    "\n",
    "# initialize optimal agent\n",
    "with open('goal_value_function.pkl', 'rb') as file:\n",
    "    value_function = pickle.load(file)\n",
    "    \n",
    "optimal_planner = OptimalGoalPolicy(\n",
    "    mdp=goal_world,\n",
    "    value_function=value_function\n",
    ")\n",
    "\n",
    "# look at the best choices for the agent\n",
    "log_likelihood_df = optimal_planner.calc_log_likelihood_all_df()\n",
    "sorted_df = log_likelihood_df.sort_values(by='Log Likelihood').reset_index(drop=True)\n",
    "\n",
    "# we can also calc the log lik of specific states\n",
    "State = TypeVar('State', bound=Hashable)\n",
    "shape = Shape(sides='square', shade='medium', texture='not_present')\n",
    "state: Tuple[Hashable, Hashable, Hashable] = (shape, shape, shape)\n",
    "#optimal_planner.calc_log_lik(state)\n",
    "sorted_df.head(36)\n",
    "value_iteration_states_nll = sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCFG Based Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PCFGGoalPolicy(GoalSelectionPolicy[State, Action]):\n",
    "    '''A policy that uses a probabilistic context-free grammar to generate goals.'''\n",
    "\n",
    "    def __init__(self, mdp: MarkovDecisionProcess[State, Action], p_rules, cap=100):\n",
    "        super().__init__(mdp)\n",
    "        self.NON_TERMINALS = [x[0] for x in p_rules]\n",
    "        self.PRODUCTIONS = {}\n",
    "        self.CAP = cap\n",
    "        self.rules = {}\n",
    "        for rule in p_rules:\n",
    "            self.PRODUCTIONS[rule[0]] = rule[1]\n",
    "\n",
    "    def generate_tree(self, logging=True, tree_str='S', log_prob=0., depth=0):\n",
    "        '''Use the production rules to generate a possible rule.'''\n",
    "        current_nt_indices = [tree_str.find(nt) for nt in self.NON_TERMINALS]\n",
    "        # Sample a non-terminal for generation\n",
    "        to_gen_idx = sample([idx for idx, el in enumerate(current_nt_indices) if el > -1], 1)[0]\n",
    "        to_gen_nt = self.NON_TERMINALS[to_gen_idx]\n",
    "        # Do generation\n",
    "        leaf = sample(self.PRODUCTIONS[to_gen_nt], 1)[0]\n",
    "        to_gen_tree_idx = tree_str.find(to_gen_nt)\n",
    "        tree_str = tree_str[:to_gen_tree_idx] + leaf + tree_str[(to_gen_tree_idx+1):]\n",
    "        # Update production log prob\n",
    "        log_prob += log(1/len(self.PRODUCTIONS[to_gen_nt]))\n",
    "        # Increase depth count\n",
    "        depth += 1\n",
    "\n",
    "        # Recursively rewrite string\n",
    "        if any (nt in tree_str for nt in self.NON_TERMINALS) and depth <= self.CAP:\n",
    "            return self.generate_tree(logging, tree_str, log_prob, depth)\n",
    "        elif any (nt in tree_str for nt in self.NON_TERMINALS):\n",
    "            if logging:\n",
    "                print('====DEPTH EXCEEDED!====')\n",
    "            return None\n",
    "        else:\n",
    "            if logging:\n",
    "                print(tree_str, log_prob)\n",
    "            return tree_str, log_prob\n",
    "        \n",
    "    def generate_rules(self, n_iterations=100000) -> pd.DataFrame:\n",
    "        '''Generate a number of rules.'''\n",
    "        # reset rules for the object\n",
    "        self.rules = []\n",
    "\n",
    "        # generate rules\n",
    "        for _ in range(n_iterations):\n",
    "            rule = self.generate_tree(logging=False)\n",
    "            if rule is not None:\n",
    "                self.rules.append(rule)\n",
    "\n",
    "        # put the rules in a dataframe\n",
    "        df = pd.DataFrame(self.rules, columns=['program', 'lp'])\n",
    "        grouped_df = df.groupby('program')['lp'].mean().reset_index()\n",
    "        count_df = df['program'].value_counts().reset_index()\n",
    "        count_df.columns = ['program', 'count']\n",
    "        result_df = pd.merge(grouped_df, count_df, on='program')\n",
    "        result_df = result_df.sort_values(by='count', ascending=False).reset_index()\n",
    "        return result_df\n",
    "    \n",
    "    def rule_applies(self, s: State, rule: str) -> bool:\n",
    "        '''Check if a rule applies to a state.'''\n",
    "        # TODO: Make a shape wrapper class to evaluate these statements\n",
    "        \n",
    "        # turn the rule from a string into a function\n",
    "        executable_string = \"s.\" + rule\n",
    "        raise NotImplementedError('Need to implement this function')\n",
    "        return exec(executable_string) \n",
    "    \n",
    "    def calculate_log_likelihood(self, s: State, rule: str) -> float:\n",
    "        '''Calculate the log likelihood of a state given a rule.'''\n",
    "\n",
    "        # check to see if the rule applies to the state\n",
    "        if self.rule_applies(s, rule):\n",
    "            rule_likelihood = self.rules[rule]\n",
    "            return log(rule_likelihood)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def calculate_log_likelihood_all(self) -> float:\n",
    "        '''Calculate the log likelihood of all states given produced rules.'''\n",
    "        # check to make sure we have rules\n",
    "        if len(self.rules) == 0:\n",
    "            raise ValueError('No rules have been generated yet!')\n",
    "        \n",
    "        # iterate through all states and calculate the log likelihood\n",
    "        log_likelihoods = {}\n",
    "\n",
    "        # loop through each state and rule\n",
    "        for state in self.state_space:\n",
    "            log_likelihood = 0\n",
    "            # we have to check if the rule applied to the state\n",
    "            for rule in self.rules:\n",
    "                # TODO: check to make sure this math is right for log lik\n",
    "                log_likelihood += rule[1] * self.calculate_log_likelihood(state, rule[0])\n",
    "            log_likelihoods[state] = log_likelihood\n",
    "\n",
    "        return log_likelihoods\n",
    "\n",
    "    def _get_rules(self) -> dict[str, list[str]]:\n",
    "        return self.rules\n",
    "    \n",
    "    def _get_rules_df(self) -> pd.DataFrame:\n",
    "        # put the rules in a dataframe\n",
    "        df = pd.DataFrame(self.rules, columns=['program', 'lp'])\n",
    "        grouped_df = df.groupby('program')['lp'].mean().reset_index()\n",
    "        count_df = df['program'].value_counts().reset_index()\n",
    "        count_df.columns = ['program', 'count']\n",
    "        result_df = pd.merge(grouped_df, count_df, on='program')\n",
    "        result_df = result_df.sort_values(by='lp', ascending=False).reset_index()\n",
    "        return result_df\n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantatie the goal space\n",
    "goal_world_pcfg = GoalWorld()\n",
    "\n",
    "# the rules of the grammar\n",
    "productions = [\n",
    "  ['S', ['and(S,S)', 'A']],\n",
    "  ['A', ['same(B,C)', 'unique(B,C)']],\n",
    "  ['B', ['everything', 'D', 'E']],\n",
    "  ['C', ['true', 'color', 'shape', 'texture', 'false']],\n",
    "  ['D', ['one', 'G']],\n",
    "  ['E', ['two', 'H']],\n",
    "  ['F', ['square', 'circle', 'triangle', 'light', 'medium', 'dark', 'plain', 'stripe']],\n",
    "  ['G', ['a', 'b', 'c']],\n",
    "  ['H', ['ab', 'ac', 'bc']],\n",
    "]\n",
    "\n",
    "pcfg_policy = PCFGGoalPolicy(mdp=goal_world_pcfg, p_rules=productions, cap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg_ll = pcfg_policy.generate_rules(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goals-concepts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
